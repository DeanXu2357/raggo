/*
Copyright Â© 2024 NAME HERE <EMAIL ADDRESS>
*/
package cmd

import (
	"bufio"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"net/url"
	"os"
	"sort"
	"strings"
	"time"

	"github.com/elastic/go-elasticsearch/v8"
	"github.com/ollama/ollama/api"
	"github.com/schollz/progressbar/v3"
	"github.com/spf13/cobra"
	"github.com/weaviate/weaviate-go-client/v4/weaviate"
	"github.com/weaviate/weaviate-go-client/v4/weaviate/graphql"
	"github.com/weaviate/weaviate/entities/models"
)

// evaluateCmd represents the evaluate command
var evaluateCmd = &cobra.Command{
	Use:   "evaluate",
	Short: "A brief description of your command",
	Long: `A longer description that spans multiple lines and likely contains examples
and usage of using your command. For example:

Cobra is a CLI library for Go that empowers applications.
This application is a tool to generate the needed files
to quickly create a Cobra application.`,
	Run: Evaluate,
}

func init() {
	rootCmd.AddCommand(evaluateCmd)
	evaluateCmd.Flags().StringP("input", "i", "", "Input JSON file path")
	_ = evaluateCmd.MarkFlagRequired("input")
	evaluateCmd.Flags().StringP("evaluate", "e", "", "Evaluation JSON file path")
	_ = evaluateCmd.MarkFlagRequired("evaluate")
	evaluateCmd.Flags().IntP("k", "k", 5, "Number of results to retrieve (default: 5)")
	evaluateCmd.Flags().BoolP("contextual", "c", false, "Add contextual information before storing in database")
	evaluateCmd.Flags().StringP("model", "m", "llama3.2:3b", "LLM model to use for generating context")
	evaluateCmd.Flags().BoolP("bm25", "b", false, "Use BM25 scoring in addition to vector search")
}

func Evaluate(cmd *cobra.Command, args []string) {
	ctx := context.Background()
	inputPath, _ := cmd.Flags().GetString("input")
	evaluatePath, _ := cmd.Flags().GetString("evaluate")
	k, _ := cmd.Flags().GetInt("k")
	useContextual, _ := cmd.Flags().GetBool("contextual")
	model, _ := cmd.Flags().GetString("model")
	useBM25, _ := cmd.Flags().GetBool("bm25")

	fmt.Printf("Starting evaluation with:\n")
	fmt.Printf("- Input file: %s\n", inputPath)
	fmt.Printf("- Evaluation file: %s\n", evaluatePath)
	fmt.Printf("- k: %d\n", k)
	fmt.Printf("- Using contextual information: %v\n", useContextual)
	fmt.Printf("- LLM model: %s\n", model)
	fmt.Printf("- Using BM25 scoring: %v\n", useBM25)

	// Generate names with timestamp
	timestamp := time.Now().Unix()
	className := fmt.Sprintf("CodeChunk_%d", timestamp)
	indexName := fmt.Sprintf("code_chunk_%d", timestamp)

	// weaviate connection
	cfg := weaviate.Config{
		Host:   "localhost:8088",
		Scheme: "http",
	}
	client, err := weaviate.NewClient(cfg)
	if err != nil {
		fmt.Printf("Failed to create Weaviate client: %v\n", err)
		return
	}

	defer func() {
		// Cleanup Weaviate class
		err := client.Schema().ClassDeleter().WithClassName(className).Do(ctx)
		if err != nil {
			fmt.Printf("Failed to cleanup Weaviate data: %v\n", err)
		}

		// Cleanup Elasticsearch index
		if useBM25 {
			resp, err := esClient.Indices.Delete([]string{indexName})
			if err != nil {
				fmt.Printf("Failed to cleanup Elasticsearch index: %v\n", err)
			}
			defer resp.Body.Close()
		}
	}()

	// load json file
	jsonFile, err := os.ReadFile(inputPath)
	if err != nil {
		fmt.Printf("Failed to read input file: %v\n", err)
		return
	}

	var codebaseRaws []CodeBaseRaw
	err = json.Unmarshal(jsonFile, &codebaseRaws)
	if err != nil {
		fmt.Printf("Failed to parse JSON: %v\n", err)
		return
	}

	// Create collections
	if err := createWeaviateCollection(ctx, client, className); err != nil {
		fmt.Println(err)
		return
	}

	if useBM25 {
		if err := createElasticsearchIndex(ctx, indexName); err != nil {
			fmt.Printf("Failed to create Elasticsearch index: %v\n", err)
			return
		}
	}

	// Calculate total chunks
	var totalChunks int
	for _, codebase := range codebaseRaws {
		totalChunks += len(codebase.Chunks)
	}

	// Create progress bar for importing
	importBar := progressbar.NewOptions(totalChunks,
		progressbar.OptionEnableColorCodes(true),
		progressbar.OptionShowCount(),
		progressbar.OptionSetDescription("[cyan]Importing chunks[reset]"),
		progressbar.OptionSetTheme(progressbar.Theme{
			Saucer:        "[green]=[reset]",
			SaucerHead:    "[green]>[reset]",
			SaucerPadding: " ",
			BarStart:      "[",
			BarEnd:        "]",
		}))

	// Import data to Weaviate
	batcher := client.Batch().ObjectsBatcher()
	objs := make([]*models.Object, 0, totalChunks)
	for _, codebase := range codebaseRaws {
		for _, chunk := range codebase.Chunks {
			importBar.Add(1)
			properties := ChunkProperties{
				DocID:        codebase.ID,
				DocUUID:      codebase.UUIDHash,
				ChunkID:      chunk.ID,
				DocIndex:     chunk.Index,
				ChunkContent: chunk.Content,
			}

			content := chunk.Content
			if useContextual {
				// Add contextual information
				contextualContent := situateContext(ctx, codebase.Content, chunk.Content, model)
				if contextualContent != "" {
					properties.ContextualContent = contextualContent
					content = fmt.Sprintf("%s\n%s", contextualContent, content)
				}
			}

			// Get embedding for the chunk content
			embedding, err := getEmbedding(content)
			if err != nil {
				fmt.Printf("\nFailed to get embedding for chunk %s: %v\n", chunk.ID, err)
				continue
			}

			// Convert properties to map for Weaviate
			propsMap := map[string]interface{}{
				"docId":        properties.DocID,
				"docUUID":      properties.DocUUID,
				"chunkId":      properties.ChunkID,
				"docIndex":     properties.DocIndex,
				"chunkContent": properties.ChunkContent,
			}
			if properties.ContextualContent != "" {
				propsMap["contextualContent"] = properties.ContextualContent
			}

			obj := &models.Object{
				Class:      className,
				Properties: propsMap,
				Vector:     embedding,
			}

			objs = append(objs, obj)
		}
	}

	// Import to Weaviate
	resp, err := batcher.WithObjects(objs...).Do(ctx)
	if err != nil {
		fmt.Printf("Failed to import data to Weaviate: %v\n", err)
		return
	}

	fmt.Printf("\nSuccessfully imported %d code chunks to Weaviate\n", len(resp))

	// Import to Elasticsearch if needed
	if useBM25 {
		// Convert objects to Elasticsearch documents
		esDocs := make([]ChunkProperties, 0, len(objs))
		for _, obj := range objs {
			props := obj.Properties.(map[string]interface{})
			doc := ChunkProperties{
				DocID:        props["docId"].(string),
				DocUUID:      props["docUUID"].(string),
				ChunkID:      props["chunkId"].(string),
				DocIndex:     props["docIndex"].(int64),
				ChunkContent: props["chunkContent"].(string),
			}
			if contextual, ok := props["contextualContent"].(string); ok {
				doc.ContextualContent = contextual
			}
			esDocs = append(esDocs, doc)
		}

		if err := indexToElasticsearch(ctx, indexName, esDocs); err != nil {
			fmt.Printf("Failed to import data to Elasticsearch: %v\n", err)
			return
		}
		fmt.Printf("Successfully imported %d code chunks to Elasticsearch\n", len(esDocs))
	}

	// Open evaluation file
	evalFile, err := os.Open(evaluatePath)
	if err != nil {
		fmt.Printf("Failed to open evaluation file: %v\n", err)
		return
	}
	defer evalFile.Close()

	const maxCapacity = 4 * 1024 * 1024 // 4MB

	// Count total evaluations
	evalScanner := bufio.NewScanner(evalFile)
	evalScanner.Buffer(make([]byte, maxCapacity), maxCapacity)
	var totalEvals int
	for evalScanner.Scan() {
		totalEvals++
	}
	evalFile.Seek(0, 0) // Reset file pointer to beginning

	// Create progress bar for evaluation
	evalBar := progressbar.NewOptions(totalEvals,
		progressbar.OptionEnableColorCodes(true),
		progressbar.OptionShowCount(),
		progressbar.OptionSetDescription("[cyan]Evaluating queries[reset]"),
		progressbar.OptionSetTheme(progressbar.Theme{
			Saucer:        "[green]=[reset]",
			SaucerHead:    "[green]>[reset]",
			SaucerPadding: " ",
			BarStart:      "[",
			BarEnd:        "]",
		}))

	// Read evaluation file line by line
	scanner := bufio.NewScanner(evalFile)
	buf := make([]byte, maxCapacity)
	scanner.Buffer(buf, maxCapacity)
	var totalScore float64
	var processedEvals int

	for scanner.Scan() {
		evalBar.Add(1)
		var evalRaw EvaluateRaw
		err := json.Unmarshal(scanner.Bytes(), &evalRaw)
		if err != nil {
			fmt.Printf("Failed to parse evaluation line: %v\n", err)
			continue
		}

		// Call RetrievalFunction with query
		retrievedChunks, err := RetrievalFunction(ctx, client, evalRaw.Query, k, className, useBM25, indexName)
		if err != nil {
			fmt.Printf("Failed to retrieve chunks for query: %v\n", err)
			continue
		}

		// Calculate score for this evaluation
		var matchCount int
		for _, retrievedChunk := range retrievedChunks {
			for _, goldenChunk := range evalRaw.GoldenChunkUUIDs {
				goldenUUID := goldenChunk.DocUUID
				goldenIndex := goldenChunk.Index

				// Compare UUID and index directly
				if retrievedChunk.DocUUID == goldenUUID && retrievedChunk.Index == goldenIndex {
					matchCount++
					break
				}
			}
		}

		score := float64(matchCount) / float64(len(evalRaw.GoldenChunkUUIDs))
		totalScore += score
		processedEvals++
	}

	if err := scanner.Err(); err != nil {
		fmt.Printf("Error reading evaluation file: %v\n", err)
		return
	}

	if processedEvals > 0 {
		averageScore := (totalScore / float64(processedEvals)) * 100
		fmt.Printf("\nEvaluation Results (k=%d):\n", k)
		fmt.Printf("Total evaluations: %d\n", processedEvals)
		fmt.Printf("Average score: %.2f%%\n", averageScore)
	} else {
		fmt.Println("No evaluations were processed")
	}

	fmt.Printf("Successfully deleted class %s\n", className)
}

func createWeaviateCollection(ctx context.Context, client *weaviate.Client, className string) error {
	schema, err := client.Schema().Getter().Do(ctx)
	if err != nil {
		return fmt.Errorf("failed to get schema: %v\n", err)
	}

	classExists := false
	for _, class := range schema.Classes {
		if class.Class == className {
			classExists = true
			break
		}
	}

	if !classExists {
		// Create Weaviate schema if not exists
		classObj := &models.Class{
			Class:      className,
			Vectorizer: "none", // We'll use custom vectors
			Properties: []*models.Property{
				{
					Name:     "docId",
					DataType: []string{"string"},
				},
				{
					Name:     "docUUID",
					DataType: []string{"string"},
				},
				{
					Name:     "chunkId",
					DataType: []string{"string"},
				},
				{
					Name:     "docIndex",
					DataType: []string{"number"},
				},
				{
					Name:     "chunkContent",
					DataType: []string{"text"},
				},
				{
					Name:     "contextualContent",
					DataType: []string{"text"},
				},
			},
		}

		// Create schema
		err = client.Schema().ClassCreator().WithClass(classObj).Do(ctx)
		if err != nil {
			return fmt.Errorf("failed to create schema: %v\n", err)
		}
		fmt.Printf("Created schema: %s\n", className)
	}

	return nil
}

type CodeBaseRaw struct {
	ID       string           `json:"doc_id"`
	UUIDHash string           `json:"original_uuid"`
	Content  string           `json:"content"`
	Chunks   []CodeBaseChunks `json:"chunks"`
}

type CodeBaseChunks struct {
	ID      string `json:"chunk_id"`
	Index   int64  `json:"original_index"`
	Content string `json:"content"`
}

type EvaluateRaw struct {
	Query            string     `json:"query"`
	GoldenChunkUUIDs []ChunkRef `json:"golden_chunk_uuids"`
}

type ChunkRef struct {
	DocUUID string
	Index   int64
}

func (e *ChunkRef) UnmarshalJSON(data []byte) error {
	var temp []interface{}

	if err := json.Unmarshal(data, &temp); err != nil {
		return err
	}

	if len(temp) != 2 {
		return fmt.Errorf("ChunkRef must have exactly 2 elements")
	}

	uuidStr, ok := temp[0].(string)
	if !ok {
		return fmt.Errorf("first element must be a string")
	}

	index, ok := temp[1].(float64)
	if !ok {
		return fmt.Errorf("second element must be a number")
	}

	e.DocUUID = uuidStr
	e.Index = int64(index)

	return nil
}

var (
	ollamaClient *api.Client
	esClient     *elasticsearch.Client
)

func init() {
	// Initialize Ollama client
	baseURL, err := url.Parse("http://localhost:11434")
	if err != nil {
		fmt.Printf("Failed to parse Ollama URL: %v\n", err)
		os.Exit(1)
	}
	ollamaClient = api.NewClient(baseURL, http.DefaultClient)

	// Initialize Elasticsearch client
	esClient, err = elasticsearch.NewClient(elasticsearch.Config{
		Addresses: []string{"http://localhost:9200"},
	})
	if err != nil {
		fmt.Printf("Failed to create Elasticsearch client: %v\n", err)
		os.Exit(1)
	}
}

type ChunkScore struct {
	DocUUID       string
	Index         int64
	ChunkID       string
	Content       string
	WeightedScore float64
}

func indexToElasticsearch(ctx context.Context, indexName string, docs []ChunkProperties) error {
	// Create bulk request body
	var buf strings.Builder
	for _, doc := range docs {
		// Create action line
		meta := map[string]interface{}{
			"index": map[string]interface{}{
				"_index": indexName,
			},
		}
		metaJSON, err := json.Marshal(meta)
		if err != nil {
			return fmt.Errorf("failed to marshal meta: %v", err)
		}
		buf.Write(metaJSON)
		buf.WriteByte('\n')

		// Create document line
		docJSON, err := json.Marshal(doc)
		if err != nil {
			return fmt.Errorf("failed to marshal document: %v", err)
		}
		buf.Write(docJSON)
		buf.WriteByte('\n')
	}

	// Send bulk request
	resp, err := esClient.Bulk(strings.NewReader(buf.String()))
	if err != nil {
		return fmt.Errorf("failed to bulk index documents: %v", err)
	}
	defer resp.Body.Close()

	if resp.IsError() {
		return fmt.Errorf("bulk indexing failed: %s", resp.String())
	}

	return nil
}

func createElasticsearchIndex(ctx context.Context, indexName string) error {
	mapping := `{
		"settings": {
			"index": {
				"queries": {
					"cache": {
						"enabled": false
					}
				},
				"similarity": {
					"default": {
						"type": "BM25"
					}
				},
				"analysis": {
					"analyzer": {
						"default": {
							"type": "english"
						}
					}
				}
			}
		},
		"mappings": {
			"properties": {
				"docId": { "type": "keyword" },
				"docUUID": { "type": "keyword" },
				"chunkId": { "type": "keyword" },
				"docIndex": { "type": "long" },
				"chunkContent": { 
					"type": "text",
					"analyzer": "english"
				},
				"contextualContent": { 
					"type": "text",
					"analyzer": "english"
				}
			}
		}
	}`

	resp, err := esClient.Indices.Create(
		indexName,
		esClient.Indices.Create.WithContext(ctx),
		esClient.Indices.Create.WithBody(strings.NewReader(mapping)),
	)
	if err != nil {
		return fmt.Errorf("failed to create index: %v", err)
	}
	defer resp.Body.Close()

	if resp.IsError() {
		return fmt.Errorf("error creating index: %s", resp.String())
	}

	return nil
}

func getEmbedding(text string) ([]float32, error) {
	// Create embeddings request
	req := api.EmbeddingRequest{
		Model:  "nomic-embed-text",
		Prompt: text,
	}

	// Call Ollama API using the client
	resp, err := ollamaClient.Embeddings(context.Background(), &req)
	if err != nil {
		return nil, fmt.Errorf("failed to get embedding: %v", err)
	}

	// Convert float64 to float32
	embedding32 := make([]float32, len(resp.Embedding))
	for i, v := range resp.Embedding {
		embedding32[i] = float32(v)
	}

	return embedding32, nil
}

func searchElasticsearch(ctx context.Context, query string, k int, indexName string) ([]ChunkScore, error) {
	// Create search request
	searchBody := map[string]interface{}{
		"size": k,
		"query": map[string]interface{}{
			"multi_match": map[string]interface{}{
				"query":  query,
				"fields": []string{"chunkContent", "contextualContent"},
			},
		},
	}

	// Convert to JSON
	searchJSON, err := json.Marshal(searchBody)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal search body: %v", err)
	}

	// Perform search
	resp, err := esClient.Search(
		esClient.Search.WithContext(ctx),
		esClient.Search.WithIndex(indexName),
		esClient.Search.WithBody(strings.NewReader(string(searchJSON))),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to perform search: %v", err)
	}
	defer resp.Body.Close()

	if resp.IsError() {
		return nil, fmt.Errorf("search failed: %s", resp.String())
	}

	// Parse response
	var result map[string]interface{}
	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, fmt.Errorf("failed to parse response: %v", err)
	}

	// Extract hits
	hits := result["hits"].(map[string]interface{})["hits"].([]interface{})
	chunks := make([]ChunkScore, 0, len(hits))

	for i, hit := range hits {
		var props ChunkProperties
		source := hit.(map[string]interface{})["_source"]
		sourceJSON, err := json.Marshal(source)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal source: %v", err)
		}
		if err := json.Unmarshal(sourceJSON, &props); err != nil {
			return nil, fmt.Errorf("failed to unmarshal properties: %v", err)
		}

		chunk := ChunkScore{
			DocUUID:       props.DocUUID,
			Index:         props.DocIndex,
			ChunkID:       props.ChunkID,
			Content:       props.ChunkContent,
			WeightedScore: ELASTIC_WEIGHT * (1.0 / float64(i+1)),
		}
		chunks = append(chunks, chunk)
	}

	return chunks, nil
}

func searchWeaviate(ctx context.Context, client *weaviate.Client, query string, k int, className string) ([]ChunkScore, error) {
	// Get embedding for query
	queryEmbedding, err := getEmbedding(query)
	if err != nil {
		return nil, fmt.Errorf("failed to get query embedding: %v", err)
	}

	// Perform semantic search using vector
	fields := []graphql.Field{
		{Name: "docUUID"},
		{Name: "chunkId"},
		{Name: "docIndex"},
		{Name: "chunkContent"},
		{Name: "contextualContent"},
	}

	result, err := client.GraphQL().Get().
		WithClassName(className).
		WithFields(fields...).
		WithNearVector(client.GraphQL().NearVectorArgBuilder().
			WithVector(queryEmbedding)).
		WithLimit(k).
		Do(ctx)

	if err != nil {
		return nil, fmt.Errorf("failed to perform search: %v", err)
	}

	// Extract results
	chunks := make([]ChunkScore, 0)
	if result.Data == nil {
		return chunks, nil
	}

	// Get the results from the GraphQL response
	if getCodeChunk, ok := result.Data["Get"].(map[string]interface{})[className].([]interface{}); ok {
		for i, item := range getCodeChunk {
			if chunk, ok := item.(map[string]interface{}); ok {
				// Parse docIndex
				docIndex, _ := chunk["docIndex"].(float64)

				chunkScore := ChunkScore{
					DocUUID:       chunk["docUUID"].(string),
					Index:         int64(docIndex),
					ChunkID:       chunk["chunkId"].(string),
					Content:       chunk["chunkContent"].(string),
					WeightedScore: WEAVIATE_WEIGHT * (1.0 / float64(i+1)),
				}
				chunks = append(chunks, chunkScore)
			}
		}
	}

	return chunks, nil
}

func mergeResults(weaviateResults, elasticResults []ChunkScore) []ChunkScore {
	// Create a map to store the highest score for each chunk
	chunkScores := make(map[string]ChunkScore)

	// Process Weaviate results
	for _, chunk := range weaviateResults {
		chunkScores[chunk.ChunkID] = chunk
	}

	// Process Elasticsearch results
	for _, chunk := range elasticResults {
		if existing, ok := chunkScores[chunk.ChunkID]; ok {
			// If chunk exists, add scores
			existing.WeightedScore += chunk.WeightedScore
			chunkScores[chunk.ChunkID] = existing
		} else {
			chunkScores[chunk.ChunkID] = chunk
		}
	}

	// Convert map back to slice
	merged := make([]ChunkScore, 0, len(chunkScores))
	for _, chunk := range chunkScores {
		merged = append(merged, chunk)
	}

	// Sort by weighted score in descending order
	sort.Slice(merged, func(i, j int) bool {
		return merged[i].WeightedScore > merged[j].WeightedScore
	})

	return merged
}

func RetrievalFunction(ctx context.Context, client *weaviate.Client, query string, k int, className string, useBM25 bool, indexName string) ([]RetrievalChunk, error) {
	// Get results from Weaviate
	weaviateResults, err := searchWeaviate(ctx, client, query, k, className)
	if err != nil {
		return nil, fmt.Errorf("failed to search Weaviate: %v", err)
	}

	var finalResults []ChunkScore
	if useBM25 {
		// Get results from Elasticsearch
		elasticResults, err := searchElasticsearch(ctx, query, k, indexName)
		if err != nil {
			return nil, fmt.Errorf("failed to search Elasticsearch: %v", err)
		}

		// Merge and sort results
		finalResults = mergeResults(weaviateResults, elasticResults)
	} else {
		finalResults = weaviateResults
	}

	// Convert to RetrievalChunk slice
	chunks := make([]RetrievalChunk, 0, len(finalResults))
	for _, result := range finalResults {
		chunks = append(chunks, RetrievalChunk{
			DocUUID: result.DocUUID,
			Index:   result.Index,
			ChunkID: result.ChunkID,
			Content: result.Content,
		})
	}

	// Limit to k results
	if len(chunks) > k {
		chunks = chunks[:k]
	}

	return chunks, nil
}

type ChunkProperties struct {
	DocID             string `json:"docId"`
	DocUUID           string `json:"docUUID"`
	ChunkID           string `json:"chunkId"`
	DocIndex          int64  `json:"docIndex"`
	ChunkContent      string `json:"chunkContent"`
	ContextualContent string `json:"contextualContent,omitempty"`
}

type RetrievalChunk struct {
	DocUUID string
	Index   int64
	ChunkID string
	Content string
}

const (
	WEAVIATE_WEIGHT = 0.8
	ELASTIC_WEIGHT  = 0.2

	DOCUMENT_CONTEXT_PROMPT = `
<document>
{doc_content}
</document>
`
	CHUNK_CONTEXT_PROMPT = `
Here is the chunk we want to situate within the whole document
<chunk>
{chunk_content}
</chunk>

Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
Answer only with the succinct context and nothing else.
`
)

func situateContext(ctx context.Context, doc, chunk string, generator string) string {
	if generator == "" {
		generator = "llama3.2:3b"
	}

	// Replace template placeholders
	docPrompt := strings.Replace(DOCUMENT_CONTEXT_PROMPT, "{doc_content}", doc, 1)
	chunkPrompt := strings.Replace(CHUNK_CONTEXT_PROMPT, "{chunk_content}", chunk, 1)

	// Combine prompts
	prompt := docPrompt + "\n" + chunkPrompt

	// Create generate request
	req := api.GenerateRequest{
		Model:  generator,
		Prompt: prompt,
	}

	var response string
	// Call Ollama API
	err := ollamaClient.Generate(ctx, &req, func(resp api.GenerateResponse) error {
		response += resp.Response
		return nil
	})

	if err != nil {
		fmt.Printf("Failed to generate context: %v\n", err)
		return ""
	}

	return response
}
